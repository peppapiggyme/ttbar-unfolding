{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as path\n",
    "import sys\n",
    "sys.path.append(path.abspath(\"src/\"))\n",
    "\n",
    "from norm_flow.realnvp import RealNVP_2D, dual_layer\n",
    "from norm_flow.utils import *\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "masks = dual_layer * 2\n",
    "hidden_dim = 16\n",
    "max_iter = 40000\n",
    "batch_size = 1024\n",
    "eval_step = 1000\n",
    "\n",
    "step = list()\n",
    "loss_train = list()\n",
    "loss_train_step = list()\n",
    "loss_val_step = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer\n",
    "realNVP = RealNVP_2D(masks, hidden_dim)\n",
    "optimizer = torch.optim.AdamW(realNVP.parameters(), lr=0.001)\n",
    "warm_up = torch.optim.lr_scheduler.LinearLR(optimizer, 1e-3, 1, 4000)\n",
    "decay = torch.optim.lr_scheduler.StepLR(optimizer, max_iter / 4, 0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ChainedScheduler([warm_up, decay])\n",
    "\n",
    "sum(p.numel() for p in realNVP.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "run_path = \"/home/zhangbw/Documents/projects/ttbar-unfolding/run\"\n",
    "file_name = \"reco_analysis__ttbar_nlo_ATLAS_PileUp.root\"\n",
    "tree_name = \"reco\" # \"truth\"\n",
    "\n",
    "df = get_dateframe(path.join(run_path, file_name), tree_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing\n",
    "quantile_scaling_(df, \"ST\")\n",
    "quantile_scaling_(df, \"b1_Pt\")\n",
    "\n",
    "print(f\"weight_median = {df.weight.median()}\")\n",
    "df[\"weight\"] /= df[\"weight\"].median()\n",
    "\n",
    "# reject outliers\n",
    "df = df[df.weight > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[[\"ST\", \"b1_Pt\"]].to_numpy()\n",
    "w = df[\"weight\"].to_numpy()\n",
    "\n",
    "x = torch.from_numpy(x).float()\n",
    "w = torch.from_numpy(w).float()\n",
    "N = x.shape[0]\n",
    "N_split = int(0.8 * N)\n",
    "x_val, w_val = x[N_split:], w[N_split:]\n",
    "x, w = x[:N_split], w[:N_split]\n",
    "N -= N_split\n",
    "\n",
    "N, x, w, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iter):\n",
    "    if i > 0 and i % eval_step == 0:\n",
    "        with torch.no_grad():\n",
    "            train_loss = np.array(loss_train[-eval_step:]).mean().item()\n",
    "            val_loss = validate(realNVP, x_val, w_val)\n",
    "            print(\n",
    "                f\"{i:6d} / {max_iter:6d}, \"\n",
    "                f\"train_loss={train_loss:.6f}, \"\n",
    "                f\"val_loss={val_loss:.6f}, \"\n",
    "                f\"lr = {scheduler.get_last_lr()[0]:.6f}\"\n",
    "            )\n",
    "            step.append(i)\n",
    "            loss_train_step.append(train_loss)\n",
    "            loss_val_step.append(val_loss)\n",
    "    xb, wb = get_batch(x, w, batch_size)\n",
    "    z, log_det = realNVP.inverse(xb)\n",
    "    loss = torch.log(two_pi) + torch.mean(wb * (torch.sum(0.5 * z**2, -1) - log_det))\n",
    "    loss_train.append(loss.item())\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(realNVP.state_dict(), path.join(run_path, \"realnvp_reco.pth\"))\n",
    "plt.figure()\n",
    "plt.plot(np.array(step), np.array(loss_train_step))\n",
    "plt.plot(np.array(step), np.array(loss_val_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_batch(x_val, w_val, 10000)\n",
    "z, log_det = realNVP.inverse(x)\n",
    "z = z.detach().numpy()\n",
    "x = x.detach().numpy()\n",
    "x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(2, figsize = (10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x[:, 0], x[:, 1], \".\")\n",
    "plt.title(\"Observed distribution\")\n",
    "plt.xlabel(r\"$S_{T}$\")\n",
    "plt.ylabel(r\"sub b-jet $p_T$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(z[:, 0], z[:, 1], \".\")\n",
    "plt.title(\"Latent distribution\")\n",
    "plt.xlabel(r\"$z_{0}$\")\n",
    "plt.ylabel(r\"$z_{1}$\")\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
